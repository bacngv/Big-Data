{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bacngv/Big-Data/blob/main/MapReduce%20Tutorials/mapreduce_with_bash.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zp5DduDsD3FB"
      },
      "source": [
        "# Mapreduce with bash\n",
        "\n",
        "In this notebook we're going to use `bash` to write a mapper and a reducer to count words in a file. This example will serve to illustrate the main features of Hadoop's MapReduce framework."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HsUM_0rlD3FF"
      },
      "source": [
        "# Table of contents\n",
        "- [What is MapReduce?](#mapreduce)\n",
        "- [The mapper](#mapper)\n",
        "    - [Test the mapper](#testmapper)\n",
        "- [Hadoop it up](#hadoop)\n",
        "    - [What is Hadoop Streaming?](#hadoopstreaming)\n",
        "    - [List your Hadoop directory](#hdfs_ls)\n",
        "    - [Test MapReduce with a dummy reducer](#dummyreducer)\n",
        "    - [Shuffling and sorting](#shuffling&sorting)\n",
        "- [The reducer](#reducer)\n",
        "    - [Test and run](#run)\n",
        "- [Run a mapreduce job with more data](#moredata)\n",
        "    - [Sort the output with `sort`](#sortoutput)\n",
        "    - [Sort the output with another MapReduce job](#sortoutputMR)\n",
        "    - [Configure sort with `KeyFieldBasedComparator`](#KeyFieldBasedComparator)\n",
        "    - [Specifying Configuration Variables with the -D Option](#configuration_variables)\n",
        "    - [What is word count useful for?](#wordcount)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7H7GhxImD3FH"
      },
      "source": [
        "## What is MapReduce? <a name=\"mapreduce\"></a>\n",
        "\n",
        "MapReduce is a computing paradigm designed to allow parallel distributed processing of massive amounts of data.\n",
        "\n",
        "Data is split across several computer nodes, there it is processed by one or more mappers. The results emitted by the mappers are first sorted and then passed to one or more reducers that process and combine the data to return the final result.\n",
        "\n",
        "![Map & Reduce](mapreduce.png)\n",
        "With [Hadoop Streaming](https://hadoop.apache.org/docs/current/hadoop-streaming/HadoopStreaming.html) it is possible to use any programming language to define a mapper and/or a reducer. Here we're going to use the Unix `bash` scripting language ([here](https://www.gnu.org/software/bash/manual/html_node/index.html) is the official documentation for the language)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBN5dpwfD3FI"
      },
      "source": [
        "## The mapper <a name=\"mapper\"></a>\n",
        "Let's write a mapper script called `map.sh`. The mapper splits each input line into words and for each word it outputs a line containing the word and `1` separated by a tab.\n",
        "\n",
        "Example: for the input\n",
        "<html>\n",
        "<pre>\n",
        "apple orange\n",
        "banana apple peach\n",
        "</pre>\n",
        "</html>\n",
        "\n",
        "`map.sh` outputs:\n",
        "<html>\n",
        "<pre>\n",
        "apple   1\n",
        "orange  1\n",
        "banana  1\n",
        "apple  1\n",
        "peach  1\n",
        "</pre>\n",
        "</html>\n",
        "\n",
        "\n",
        "The <a href=\"https://ipython.readthedocs.io/en/stable/interactive/magics.html\">_cell magic_</a> [`%%writefile`](https://ipython.readthedocs.io/en/stable/interactive/magics.html#cellmagic-writefile) allows us to write the contents of the cell to a file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p9YlfOSBD3FJ",
        "outputId": "eed23404-ea22-4f04-876d-df0f13adf589"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing map.sh\n"
          ]
        }
      ],
      "source": [
        "%%writefile map.sh\n",
        "#!/bin/bash\n",
        "\n",
        "while read line\n",
        "do\n",
        " for word in $line\n",
        " do\n",
        "  if [ -n \"$word\" ]\n",
        "  then\n",
        "     echo -e ${word}\"\\t1\"\n",
        "  fi\n",
        " done\n",
        "done"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1jR1XrtD3FK"
      },
      "source": [
        "After running the cell above, you should have a new file `map.sh` in your current directory.\n",
        "The file can be seen in the left panel of JupyterLab or by using a list command on the bash command-line.\n",
        "\n",
        "**Note:** you can execute a single bash command in a Jupyter notebook cell by prepending an exclamation point to the command."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DbPU5lxuD3FK",
        "outputId": "cf64448f-c30b-4f5c-abc8-85e990362e23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-rw-r--r-- 1 root root 124 Apr 27 15:26 map.sh\n"
          ]
        }
      ],
      "source": [
        "!ls -hl map.sh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ix9IGABpD3FL"
      },
      "source": [
        "### Test the mapper <a name=\"testmapper\"></a>\n",
        "We're going to test the mapper on on the command line with a small text file `fruits.txt` by first creating the text file.\n",
        "In this file `apple` for instance appears two times, that's what we want our mapreduce job to compute."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NcgqEXw5D3FL",
        "outputId": "0c93b691-e150-4395-f062-a15e7715a1be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing fruits.txt\n"
          ]
        }
      ],
      "source": [
        "%%writefile fruits.txt\n",
        "apple banana\n",
        "peach orange peach peach\n",
        "pineapple peach apple"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UgOUZXxvD3FL",
        "outputId": "9ba24c0d-f249-4eca-f4e4-b2b4ecda6993"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "apple banana\n",
            "peach orange peach peach\n",
            "pineapple peach apple\n"
          ]
        }
      ],
      "source": [
        "!cat fruits.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQFCbVD7D3FM"
      },
      "source": [
        "Test the mapper"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod +x map.sh"
      ],
      "metadata": {
        "id": "nBcalfBCELf7"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8vqlbtWND3FM",
        "outputId": "650b8ef2-5f6f-4a9b-9b29-c642b91ab9cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "apple\t1\n",
            "banana\t1\n",
            "peach\t1\n",
            "orange\t1\n",
            "peach\t1\n",
            "peach\t1\n",
            "pineapple\t1\n",
            "peach\t1\n",
            "apple\t1\n"
          ]
        }
      ],
      "source": [
        "!cat fruits.txt|./map.sh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cGxPiebD3FN"
      },
      "source": [
        "If the script `map.sh` does not have the executable bit set, you need to set the correct permissions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "f-LJFX_8D3FN"
      },
      "outputs": [],
      "source": [
        "!chmod 700 map.sh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OD_oCJq1D3FO"
      },
      "source": [
        "## Hadoop it up <a name=\"hadoop\"></a>\n",
        "Let us now run a MapReduce job with Hadoop Streaming."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "HADOOP_URL = \"https://dlcdn.apache.org/hadoop/common/stable/hadoop-3.4.0.tar.gz\"\n",
        "\n",
        "import requests\n",
        "import os\n",
        "import tarfile\n",
        "\n",
        "def download_and_extract_targz(url):\n",
        "    response = requests.get(url)\n",
        "    filename = url.rsplit('/', 1)[-1]\n",
        "    HADOOP_HOME = filename[:-7]\n",
        "    # set HADOOP_HOME environment variable\n",
        "    os.environ['HADOOP_HOME'] = HADOOP_HOME\n",
        "    if os.path.isdir(HADOOP_HOME):\n",
        "      print(\"Not downloading, Hadoop folder {} already exists\".format(HADOOP_HOME))\n",
        "      return\n",
        "    if response.status_code == 200:\n",
        "        with open(filename, 'wb') as file:\n",
        "            file.write(response.content)\n",
        "        with tarfile.open(filename, 'r:gz') as tar_ref:\n",
        "            extract_path = tar_ref.extractall(path='.')\n",
        "            # Get the names of all members (files and directories) in the archive\n",
        "            all_members = tar_ref.getnames()\n",
        "            # If there is a top-level directory, get its name\n",
        "            if all_members:\n",
        "              top_level_directory = all_members[0]\n",
        "              print(f\"ZIP file downloaded and extracted successfully. Contents saved at: {top_level_directory}\")\n",
        "    else:\n",
        "        print(f\"Failed to download ZIP file. Status code: {response.status_code}\")\n",
        "\n",
        "\n",
        "download_and_extract_targz(HADOOP_URL)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VWr2LbuuGk8a",
        "outputId": "4ed28128-42d1-474b-8ffd-26a752d4ae6a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ZIP file downloaded and extracted successfully. Contents saved at: hadoop-3.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# HADOOP_HOME was set earlier when downloading Hadoop distribution\n",
        "print(\"HADOOP_HOME is {}\".format(os.environ['HADOOP_HOME']))\n",
        "\n",
        "os.environ['PATH'] = ':'.join([os.path.join(os.environ['HADOOP_HOME'], 'bin'), os.environ['PATH']])\n",
        "print(\"PATH is {}\".format(os.environ['PATH']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j4I0stzyHFvZ",
        "outputId": "ab9f7129-f232-494c-f20c-ccc6cea80ab9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HADOOP_HOME is hadoop-3.4.0\n",
            "PATH is hadoop-3.4.0/bin:/opt/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "# set variable JAVA_HOME (install Java if necessary)\n",
        "def is_java_installed():\n",
        "    os.environ['JAVA_HOME'] = os.path.realpath(shutil.which(\"java\")).split('/bin')[0]\n",
        "    return os.environ['JAVA_HOME']\n",
        "\n",
        "def install_java():\n",
        "    # Uncomment and modify the desired version\n",
        "    # java_version= 'openjdk-11-jre-headless'\n",
        "    # java_version= 'default-jre'\n",
        "    # java_version= 'openjdk-17-jre-headless'\n",
        "    # java_version= 'openjdk-18-jre-headless'\n",
        "    java_version= 'openjdk-19-jre-headless'\n",
        "\n",
        "    print(f\"Java not found. Installing {java_version} ... (this might take a while)\")\n",
        "    try:\n",
        "        cmd = f\"apt install -y {java_version}\"\n",
        "        subprocess_output = subprocess.run(cmd, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
        "        stdout_result = subprocess_output.stdout\n",
        "        # Process the results as needed\n",
        "        print(\"Done installing Java {}\".format(java_version))\n",
        "        os.environ['JAVA_HOME'] = os.path.realpath(shutil.which(\"java\")).split('/bin')[0]\n",
        "        print(\"JAVA_HOME is {}\".format(os.environ['JAVA_HOME']))\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        # Handle the error if the command returns a non-zero exit code\n",
        "        print(\"Command failed with return code {}\".format(e.returncode))\n",
        "        print(\"stdout: {}\".format(e.stdout))\n",
        "\n",
        "# Install Java if not available\n",
        "if is_java_installed():\n",
        "    print(\"Java is already installed: {}\".format(os.environ['JAVA_HOME']))\n",
        "else:\n",
        "    print(\"Installing Java\")\n",
        "    install_java()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3_3ywvTHIlo",
        "outputId": "09c97a58-6e29-46f4-eca1-127ef6e52497"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Java is already installed: /usr/lib/jvm/java-11-openjdk-amd64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXsP40sHD3FO"
      },
      "source": [
        "### What is Hadoop Streaming <a name=\"hadoopstreaming\"></a>\n",
        "\n",
        "Hadoop Streaming is a library included in the Hadoop distribution that enables you to develop MapReduce executables in languages other than Java.\n",
        "\n",
        "Mapper and/or reducer can be any sort of executables that read the input from stdin and emit the output to stdout. By default, input is read line by line and the prefix of a line up to the first tab character is the key; the rest of the line (excluding the tab character) will be the value.\n",
        "\n",
        "If there is no tab character in the line, then the entire line is considered as key and the value is null. The default input format is specified in the class `TextInputFormat` (see the [API documentation](https://hadoop.apache.org/docs/stable/api/org/apache/hadoop/mapred/TextInputFormat.html)) but this can can be customized for instance by defining another field separator (see the [Hadoop Streaming documentation](https://hadoop.apache.org/docs/stable/hadoop-streaming/HadoopStreaming.html#Customizing_How_Lines_are_Split_into_KeyValue_Pairs).\n",
        "\n",
        "This is an example of MapReduce streaming invocation syntax:\n",
        "<html>\n",
        "<pre>\n",
        "    mapred streaming \\\n",
        "  -input myInputDirs \\\n",
        "  -output myOutputDir \\\n",
        "  -mapper /bin/cat \\\n",
        "  -reducer /usr/bin/wc\n",
        "\n",
        "</pre>\n",
        "</html>\n",
        "\n",
        "You can find the full official documentation for Hadoop Streaming from Apache Hadoop here: [https://hadoop.apache.org/docs/stable/hadoop-streaming/HadoopStreaming.html](https://hadoop.apache.org/docs/stable/hadoop-streaming/HadoopStreaming.html).\n",
        "\n",
        "All options for the Hadoop Streaming command are described here: [Streaming Command Options](https://hadoop.apache.org/docs/current/hadoop-streaming/HadoopStreaming.html#Streaming_Command_Options) and can be listed with the command"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZB78UK9oD3FO",
        "outputId": "c6b7b675-7295-4fae-b21d-f306af6498a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Usage: $HADOOP_HOME/bin/hadoop jar hadoop-streaming.jar [options]\n",
            "Options:\n",
            "  -input          <path> DFS input file(s) for the Map step.\n",
            "  -output         <path> DFS output directory for the Reduce step.\n",
            "  -mapper         <cmd|JavaClassName> Optional. Command to be run as mapper.\n",
            "  -combiner       <cmd|JavaClassName> Optional. Command to be run as combiner.\n",
            "  -reducer        <cmd|JavaClassName> Optional. Command to be run as reducer.\n",
            "  -file           <file> Optional. File/dir to be shipped in the Job jar file.\n",
            "                  Deprecated. Use generic option \"-files\" instead.\n",
            "  -inputformat    <TextInputFormat(default)|SequenceFileAsTextInputFormat|JavaClassName>\n",
            "                  Optional. The input format class.\n",
            "  -outputformat   <TextOutputFormat(default)|JavaClassName>\n",
            "                  Optional. The output format class.\n",
            "  -partitioner    <JavaClassName>  Optional. The partitioner class.\n",
            "  -numReduceTasks <num> Optional. Number of reduce tasks.\n",
            "  -inputreader    <spec> Optional. Input recordreader spec.\n",
            "  -cmdenv         <n>=<v> Optional. Pass env.var to streaming commands.\n",
            "  -mapdebug       <cmd> Optional. To run this script when a map task fails.\n",
            "  -reducedebug    <cmd> Optional. To run this script when a reduce task fails.\n",
            "  -io             <identifier> Optional. Format to use for input to and output\n",
            "                  from mapper/reducer commands\n",
            "  -lazyOutput     Optional. Lazily create Output.\n",
            "  -background     Optional. Submit the job and don't wait till it completes.\n",
            "  -verbose        Optional. Print verbose output.\n",
            "  -info           Optional. Print detailed usage.\n",
            "  -help           Optional. Print help message.\n",
            "\n",
            "Generic options supported are:\n",
            "-conf <configuration file>        specify an application configuration file\n",
            "-D <property=value>               define a value for a given property\n",
            "-fs <file:///|hdfs://namenode:port> specify default filesystem URL to use, overrides 'fs.defaultFS' property from configurations.\n",
            "-jt <local|resourcemanager:port>  specify a ResourceManager\n",
            "-files <file1,...>                specify a comma-separated list of files to be copied to the map reduce cluster\n",
            "-libjars <jar1,...>               specify a comma-separated list of jar files to be included in the classpath\n",
            "-archives <archive1,...>          specify a comma-separated list of archives to be unarchived on the compute machines\n",
            "\n",
            "The general command line syntax is:\n",
            "command [genericOptions] [commandOptions]\n",
            "\n",
            "\n",
            "For more details about these options:\n",
            "Use $HADOOP_HOME/bin/hadoop jar hadoop-streaming.jar -info\n"
          ]
        }
      ],
      "source": [
        "!mapred streaming --help"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9-801XPD3FO"
      },
      "source": [
        "Now in order to run a mapreduce job that we need to \"upload\" the input file to the Hadoop file system."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_3qJpg_D3FO"
      },
      "source": [
        "### List your Hadoop directory <a name=\"hdfs_ls\"></a>\n",
        "\n",
        "With the command `hdfs dfs -l` you can view the content of your HDFS home directory.\n",
        "\n",
        "`hdfs dfs` you can run a filesystem command on the Hadoop fileystem. The complete list of commands can be found in the [System Shell Guide](https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HDFSCommands.html#dfs)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hdGlKZ_ND3FO",
        "outputId": "086250f0-55c3-4b05-89a7-7d565e17a2b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 6 items\n",
            "drwxr-xr-x   - root root       4096 2024-04-25 13:24 .config\n",
            "-rw-r--r--   1 root root         60 2024-04-27 15:26 fruits.txt\n",
            "drwxr-xr-x   - root root       4096 2024-03-04 08:05 hadoop-3.4.0\n",
            "-rw-r--r--   1 root root  965537117 2024-04-27 15:26 hadoop-3.4.0.tar.gz\n",
            "-rwx------   1 root root        124 2024-04-27 15:26 map.sh\n",
            "drwxr-xr-x   - root root       4096 2024-04-25 13:25 sample_data\n"
          ]
        }
      ],
      "source": [
        "!hdfs dfs -ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHanZjE3D3FP"
      },
      "source": [
        "Now create a directory `wordcount` with a subdirectory `input` on the Hadoop filesystem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "dmt_6ylDD3FP"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "hdfs dfs -mkdir -p wordcount"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKXvmij-D3FP"
      },
      "source": [
        "Copy the file fruits.txt to Hadoop in the folder `wordcount/input`.\n",
        "\n",
        "Why do we need this step? Because the file `fruits.txt` needs to reside on the Hadoop filesystem in order to enjoy of all of the features of Hadoop (data partitioning, distributed processing, fault tolerance)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "GVqFURuyD3FP"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "hdfs dfs -rm -r wordcount/input 2>/dev/null\n",
        "hdfs dfs -mkdir wordcount/input\n",
        "hdfs dfs -put fruits.txt wordcount/input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qucg8xRRD3FP"
      },
      "source": [
        "Let's check if the file is there now.\n",
        "\n",
        "**Note:** it is convenient use the option `-h` for `ls` to show file sizes in human-readable form (showing sizes in Kilobytes, Megabytes, Gigabytes, etc.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XOtBLiJCD3FP",
        "outputId": "3fbb0185-2dd3-426a-918c-f7c94f12bf57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-rw-r--r--   1 root root         60 2024-04-27 15:27 wordcount/input/fruits.txt\n"
          ]
        }
      ],
      "source": [
        "!hdfs dfs -ls -h -R wordcount/input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUrZeJN5D3FP"
      },
      "source": [
        "### Test MapReduce with a dummy reducer <a name=\"dummyreducer\"></a>\n",
        "\n",
        "Let's try to run the mapper using a dummy reducer (`/bin/cat` does nothing else than echoing the data it receives).\n",
        "\n",
        "**Warning:** mapreduce tends to produce a verbose output, so be ready to see a long output. What you should look for is a message of the kind <html><pre>\"INFO mapreduce.Job: Job ... completed successfully\"</pre></html>\n",
        "\n",
        "**Note:** at the beginning of next cell you'll see a command `hadoop fs -rmr wordcount/output 2>/dev/null`. This is needed because when you run a job several times mapreduce will give an error if you try to overwrite the same output directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ut5tsrcED3FQ",
        "outputId": "8697238d-d2fb-49b2-d53d-6daecb120293"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-04-27 15:27:41,381 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2024-04-27 15:27:41,579 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2024-04-27 15:27:41,579 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2024-04-27 15:27:41,603 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2024-04-27 15:27:41,951 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2024-04-27 15:27:41,973 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2024-04-27 15:27:42,390 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local415472041_0001\n",
            "2024-04-27 15:27:42,390 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2024-04-27 15:27:42,784 INFO mapred.LocalDistributedCacheManager: Localized file:/content/map.sh as file:/tmp/hadoop-root/mapred/local/job_local415472041_0001_2e0d7d7b-5c38-48fe-9a60-c0c921fb67b5/map.sh\n",
            "2024-04-27 15:27:42,938 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2024-04-27 15:27:42,940 INFO mapreduce.Job: Running job: job_local415472041_0001\n",
            "2024-04-27 15:27:42,946 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2024-04-27 15:27:42,948 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2024-04-27 15:27:42,957 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-04-27 15:27:42,957 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-04-27 15:27:43,014 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2024-04-27 15:27:43,019 INFO mapred.LocalJobRunner: Starting task: attempt_local415472041_0001_m_000000_0\n",
            "2024-04-27 15:27:43,064 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-04-27 15:27:43,064 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-04-27 15:27:43,087 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-04-27 15:27:43,101 INFO mapred.MapTask: Processing split: file:/content/wordcount/input/fruits.txt:0+60\n",
            "2024-04-27 15:27:43,118 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2024-04-27 15:27:43,186 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2024-04-27 15:27:43,186 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2024-04-27 15:27:43,187 INFO mapred.MapTask: soft limit at 83886080\n",
            "2024-04-27 15:27:43,187 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2024-04-27 15:27:43,187 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2024-04-27 15:27:43,190 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2024-04-27 15:27:43,200 INFO streaming.PipeMapRed: PipeMapRed exec [/content/./map.sh]\n",
            "2024-04-27 15:27:43,206 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2024-04-27 15:27:43,208 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2024-04-27 15:27:43,209 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2024-04-27 15:27:43,210 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2024-04-27 15:27:43,211 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2024-04-27 15:27:43,212 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2024-04-27 15:27:43,214 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2024-04-27 15:27:43,215 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2024-04-27 15:27:43,215 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2024-04-27 15:27:43,215 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2024-04-27 15:27:43,216 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2024-04-27 15:27:43,217 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2024-04-27 15:27:43,240 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2024-04-27 15:27:43,245 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2024-04-27 15:27:43,248 INFO streaming.PipeMapRed: Records R/W=3/1\n",
            "2024-04-27 15:27:43,249 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2024-04-27 15:27:43,252 INFO mapred.LocalJobRunner: \n",
            "2024-04-27 15:27:43,252 INFO mapred.MapTask: Starting flush of map output\n",
            "2024-04-27 15:27:43,252 INFO mapred.MapTask: Spilling map output\n",
            "2024-04-27 15:27:43,252 INFO mapred.MapTask: bufstart = 0; bufend = 78; bufvoid = 104857600\n",
            "2024-04-27 15:27:43,252 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214364(104857456); length = 33/6553600\n",
            "2024-04-27 15:27:43,260 INFO mapred.MapTask: Finished spill 0\n",
            "2024-04-27 15:27:43,274 INFO mapred.Task: Task:attempt_local415472041_0001_m_000000_0 is done. And is in the process of committing\n",
            "2024-04-27 15:27:43,277 INFO mapred.LocalJobRunner: Records R/W=3/1\n",
            "2024-04-27 15:27:43,278 INFO mapred.Task: Task 'attempt_local415472041_0001_m_000000_0' done.\n",
            "2024-04-27 15:27:43,285 INFO mapred.Task: Final Counters for attempt_local415472041_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=142117\n",
            "\t\tFILE: Number of bytes written=858338\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=3\n",
            "\t\tMap output records=9\n",
            "\t\tMap output bytes=78\n",
            "\t\tMap output materialized bytes=102\n",
            "\t\tInput split bytes=92\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=9\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=380633088\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=76\n",
            "2024-04-27 15:27:43,285 INFO mapred.LocalJobRunner: Finishing task: attempt_local415472041_0001_m_000000_0\n",
            "2024-04-27 15:27:43,285 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2024-04-27 15:27:43,289 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2024-04-27 15:27:43,289 INFO mapred.LocalJobRunner: Starting task: attempt_local415472041_0001_r_000000_0\n",
            "2024-04-27 15:27:43,298 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-04-27 15:27:43,298 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-04-27 15:27:43,299 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-04-27 15:27:43,306 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@148c1519\n",
            "2024-04-27 15:27:43,308 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2024-04-27 15:27:43,358 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2382574336, maxSingleShuffleLimit=595643584, mergeThreshold=1572499072, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2024-04-27 15:27:43,378 INFO reduce.EventFetcher: attempt_local415472041_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2024-04-27 15:27:43,428 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local415472041_0001_m_000000_0 decomp: 98 len: 102 to MEMORY\n",
            "2024-04-27 15:27:43,432 INFO reduce.InMemoryMapOutput: Read 98 bytes from map-output for attempt_local415472041_0001_m_000000_0\n",
            "2024-04-27 15:27:43,437 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 98, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->98\n",
            "2024-04-27 15:27:43,441 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2024-04-27 15:27:43,442 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-04-27 15:27:43,442 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2024-04-27 15:27:43,452 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2024-04-27 15:27:43,452 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 90 bytes\n",
            "2024-04-27 15:27:43,454 INFO reduce.MergeManagerImpl: Merged 1 segments, 98 bytes to disk to satisfy reduce memory limit\n",
            "2024-04-27 15:27:43,455 INFO reduce.MergeManagerImpl: Merging 1 files, 102 bytes from disk\n",
            "2024-04-27 15:27:43,455 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2024-04-27 15:27:43,455 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2024-04-27 15:27:43,456 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 90 bytes\n",
            "2024-04-27 15:27:43,457 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-04-27 15:27:43,458 INFO streaming.PipeMapRed: PipeMapRed exec [/bin/cat]\n",
            "2024-04-27 15:27:43,462 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
            "2024-04-27 15:27:43,469 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
            "2024-04-27 15:27:43,484 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2024-04-27 15:27:43,487 INFO streaming.PipeMapRed: Records R/W=9/1\n",
            "2024-04-27 15:27:43,488 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2024-04-27 15:27:43,488 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2024-04-27 15:27:43,489 INFO mapred.Task: Task:attempt_local415472041_0001_r_000000_0 is done. And is in the process of committing\n",
            "2024-04-27 15:27:43,490 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-04-27 15:27:43,491 INFO mapred.Task: Task attempt_local415472041_0001_r_000000_0 is allowed to commit now\n",
            "2024-04-27 15:27:43,492 INFO output.FileOutputCommitter: Saved output of task 'attempt_local415472041_0001_r_000000_0' to file:/content/wordcount/output\n",
            "2024-04-27 15:27:43,494 INFO mapred.LocalJobRunner: Records R/W=9/1 > reduce\n",
            "2024-04-27 15:27:43,495 INFO mapred.Task: Task 'attempt_local415472041_0001_r_000000_0' done.\n",
            "2024-04-27 15:27:43,499 INFO mapred.Task: Final Counters for attempt_local415472041_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=142353\n",
            "\t\tFILE: Number of bytes written=858530\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=5\n",
            "\t\tReduce shuffle bytes=102\n",
            "\t\tReduce input records=9\n",
            "\t\tReduce output records=9\n",
            "\t\tSpilled Records=9\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=10\n",
            "\t\tTotal committed heap usage (bytes)=380633088\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=90\n",
            "2024-04-27 15:27:43,499 INFO mapred.LocalJobRunner: Finishing task: attempt_local415472041_0001_r_000000_0\n",
            "2024-04-27 15:27:43,499 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2024-04-27 15:27:43,945 INFO mapreduce.Job: Job job_local415472041_0001 running in uber mode : false\n",
            "2024-04-27 15:27:43,946 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2024-04-27 15:27:43,948 INFO mapreduce.Job: Job job_local415472041_0001 completed successfully\n",
            "2024-04-27 15:27:43,969 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=284470\n",
            "\t\tFILE: Number of bytes written=1716868\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=3\n",
            "\t\tMap output records=9\n",
            "\t\tMap output bytes=78\n",
            "\t\tMap output materialized bytes=102\n",
            "\t\tInput split bytes=92\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=5\n",
            "\t\tReduce shuffle bytes=102\n",
            "\t\tReduce input records=9\n",
            "\t\tReduce output records=9\n",
            "\t\tSpilled Records=18\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=10\n",
            "\t\tTotal committed heap usage (bytes)=761266176\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=76\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=90\n",
            "2024-04-27 15:27:43,969 INFO streaming.StreamJob: Output directory: wordcount/output\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "hdfs dfs -rm -r wordcount/output 2>/dev/null\n",
        "mapred streaming \\\n",
        "  -files map.sh \\\n",
        "  -input wordcount/input \\\n",
        "  -output wordcount/output \\\n",
        "  -mapper map.sh \\\n",
        "  -reducer /bin/cat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aiflR496D3FR"
      },
      "source": [
        "The output of the mapreduce job is in the `output` subfolder of the input directory. Let's check what's inside it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HXE5elH9D3FR",
        "outputId": "9bae4297-b6f0-4dff-c1eb-dc50bd215fbd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2 items\n",
            "-rw-r--r--   1 root root          0 2024-04-27 15:27 wordcount/output/_SUCCESS\n",
            "-rw-r--r--   1 root root         78 2024-04-27 15:27 wordcount/output/part-00000\n"
          ]
        }
      ],
      "source": [
        "!hdfs dfs -ls wordcount/output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSVTe1_jD3FR"
      },
      "source": [
        "If `output` contains a file named `_SUCCESS` that means that the mapreduce job completed successfully.\n",
        "\n",
        "**Note:** when dealing with Big Data it's always advisable to pipe the output of `cat` commands to `head` (or `tail`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aOiDVpQDD3FR",
        "outputId": "63afc977-18fd-4ce0-8b52-a1581fdee40c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "apple\t1\n",
            "apple\t1\n",
            "banana\t1\n",
            "orange\t1\n",
            "peach\t1\n",
            "peach\t1\n",
            "peach\t1\n",
            "peach\t1\n",
            "pineapple\t1\n"
          ]
        }
      ],
      "source": [
        "!hdfs dfs -cat wordcount/output/part*|head"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOpW1oAcD3FS"
      },
      "source": [
        "We have gotten as expected all the output from the mapper. Something worth of notice is that the data outputted from the mapper _**has been sorted**_. We haven't asked for that but this step is automatically performed by the mapper as soon as the number of reducers is $\\gt 0$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJFVuyTAD3FS"
      },
      "source": [
        "### Shuffling and sorting <a name=\"shuffling&sorting\"></a>\n",
        "The following picture illustrates the concept of shuffling and sorting that is automatically performed by Hadoop after each map before passing the output to reduce. In the picture the outputs of the two mapper tasks are shown. The arrows represent shuffling and sorting done before delivering the data to one reducer (rightmost box).\n",
        "![Shuffle & sort](shuffle_sort.png)\n",
        "The shuffling and sorting phase is often one of the most costly in a MapReduce job.\n",
        "\n",
        "\n",
        "<b>Note:</b> the job ran with two mappers because $2$ is the default number of mappers in Hadoop."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OLEKdNMD3FS"
      },
      "source": [
        "## The reducer <a name=\"reducer\"></a>\n",
        "Let's now write a reducer script called `reduce.sh`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GeXiCVmDD3FS",
        "outputId": "93b56e8e-696b-4dd4-c325-847cc0459e73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing reduce.sh\n"
          ]
        }
      ],
      "source": [
        "%%writefile reduce.sh\n",
        "#!/bin/bash\n",
        "\n",
        "currkey=\"\"\n",
        "currcount=0\n",
        "while IFS=$'\\t' read -r key val\n",
        "do\n",
        "  if [[ $key == $currkey ]]\n",
        "  then\n",
        "      currcount=$(( currcount + val ))\n",
        "  else\n",
        "    if [ -n \"$currkey\" ]\n",
        "    then\n",
        "      echo -e ${currkey} \"\\t\" ${currcount}\n",
        "    fi\n",
        "    currkey=$key\n",
        "    currcount=1\n",
        "  fi\n",
        "done\n",
        "# last one\n",
        "echo -e ${currkey} \"\\t\" ${currcount}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_inmhPDGD3FS"
      },
      "source": [
        "Set permission for the reducer script."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "fqA0Ag2yD3FS"
      },
      "outputs": [],
      "source": [
        "!chmod 700 reduce.sh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SaD2UV0QD3FT"
      },
      "source": [
        "### Test and run <a name=\"run\"></a>\n",
        "\n",
        "Test map and reduce on the shell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49VknYKbD3FU",
        "outputId": "4a03ea32-ca78-4b00-f9f1-73135845f20e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "apple \t 2\n",
            "banana \t 1\n",
            "orange \t 1\n",
            "peach \t 4\n",
            "pineapple \t 1\n"
          ]
        }
      ],
      "source": [
        "!cat fruits.txt|./map.sh|sort|./reduce.sh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m05eooyDD3FU"
      },
      "source": [
        "Once we've made sure that the reducer script runs correctly on the shell, we can run it on the cluster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qX_In5JID3FU",
        "outputId": "97ae4016-a16a-41bd-dda5-90c79d0653c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleted wordcount/output\n",
            "packageJobJar: [map.sh, reduce.sh] [] /tmp/streamjob3818772969888583807.jar tmpDir=null\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-04-27 15:27:52,545 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
            "2024-04-27 15:27:53,339 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2024-04-27 15:27:53,501 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2024-04-27 15:27:53,501 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2024-04-27 15:27:53,526 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2024-04-27 15:27:53,765 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2024-04-27 15:27:53,794 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2024-04-27 15:27:54,108 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local474333698_0001\n",
            "2024-04-27 15:27:54,108 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2024-04-27 15:27:54,546 INFO mapred.LocalDistributedCacheManager: Localized file:/content/map.sh as file:/tmp/hadoop-root/mapred/local/job_local474333698_0001_5750d848-0bb6-495b-b90b-3b8c70597132/map.sh\n",
            "2024-04-27 15:27:54,581 INFO mapred.LocalDistributedCacheManager: Localized file:/content/reduce.sh as file:/tmp/hadoop-root/mapred/local/job_local474333698_0001_dc445a6a-f616-45ed-aee0-efcff7cd7fcf/reduce.sh\n",
            "2024-04-27 15:27:54,724 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2024-04-27 15:27:54,729 INFO mapreduce.Job: Running job: job_local474333698_0001\n",
            "2024-04-27 15:27:54,736 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2024-04-27 15:27:54,739 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2024-04-27 15:27:54,751 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-04-27 15:27:54,752 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-04-27 15:27:54,809 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2024-04-27 15:27:54,817 INFO mapred.LocalJobRunner: Starting task: attempt_local474333698_0001_m_000000_0\n",
            "2024-04-27 15:27:54,848 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-04-27 15:27:54,851 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-04-27 15:27:54,875 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-04-27 15:27:54,894 INFO mapred.MapTask: Processing split: file:/content/wordcount/input/fruits.txt:0+60\n",
            "2024-04-27 15:27:54,917 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2024-04-27 15:27:55,012 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2024-04-27 15:27:55,012 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2024-04-27 15:27:55,012 INFO mapred.MapTask: soft limit at 83886080\n",
            "2024-04-27 15:27:55,013 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2024-04-27 15:27:55,013 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2024-04-27 15:27:55,016 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2024-04-27 15:27:55,023 INFO streaming.PipeMapRed: PipeMapRed exec [/content/./map.sh]\n",
            "2024-04-27 15:27:55,030 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2024-04-27 15:27:55,033 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2024-04-27 15:27:55,033 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2024-04-27 15:27:55,034 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2024-04-27 15:27:55,037 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2024-04-27 15:27:55,044 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2024-04-27 15:27:55,046 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2024-04-27 15:27:55,046 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2024-04-27 15:27:55,046 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2024-04-27 15:27:55,047 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2024-04-27 15:27:55,048 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2024-04-27 15:27:55,049 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2024-04-27 15:27:55,086 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2024-04-27 15:27:55,095 INFO streaming.PipeMapRed: Records R/W=3/1\n",
            "2024-04-27 15:27:55,096 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2024-04-27 15:27:55,096 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2024-04-27 15:27:55,099 INFO mapred.LocalJobRunner: \n",
            "2024-04-27 15:27:55,099 INFO mapred.MapTask: Starting flush of map output\n",
            "2024-04-27 15:27:55,099 INFO mapred.MapTask: Spilling map output\n",
            "2024-04-27 15:27:55,099 INFO mapred.MapTask: bufstart = 0; bufend = 78; bufvoid = 104857600\n",
            "2024-04-27 15:27:55,099 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214364(104857456); length = 33/6553600\n",
            "2024-04-27 15:27:55,108 INFO mapred.MapTask: Finished spill 0\n",
            "2024-04-27 15:27:55,142 INFO mapred.Task: Task:attempt_local474333698_0001_m_000000_0 is done. And is in the process of committing\n",
            "2024-04-27 15:27:55,146 INFO mapred.LocalJobRunner: Records R/W=3/1\n",
            "2024-04-27 15:27:55,147 INFO mapred.Task: Task 'attempt_local474333698_0001_m_000000_0' done.\n",
            "2024-04-27 15:27:55,156 INFO mapred.Task: Final Counters for attempt_local474333698_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=1193\n",
            "\t\tFILE: Number of bytes written=716554\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=3\n",
            "\t\tMap output records=9\n",
            "\t\tMap output bytes=78\n",
            "\t\tMap output materialized bytes=102\n",
            "\t\tInput split bytes=92\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=9\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=11\n",
            "\t\tTotal committed heap usage (bytes)=419430400\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=76\n",
            "2024-04-27 15:27:55,156 INFO mapred.LocalJobRunner: Finishing task: attempt_local474333698_0001_m_000000_0\n",
            "2024-04-27 15:27:55,156 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2024-04-27 15:27:55,169 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2024-04-27 15:27:55,169 INFO mapred.LocalJobRunner: Starting task: attempt_local474333698_0001_r_000000_0\n",
            "2024-04-27 15:27:55,192 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-04-27 15:27:55,192 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-04-27 15:27:55,193 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-04-27 15:27:55,196 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@4dd5842f\n",
            "2024-04-27 15:27:55,198 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2024-04-27 15:27:55,222 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2382574336, maxSingleShuffleLimit=595643584, mergeThreshold=1572499072, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2024-04-27 15:27:55,237 INFO reduce.EventFetcher: attempt_local474333698_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2024-04-27 15:27:55,278 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local474333698_0001_m_000000_0 decomp: 98 len: 102 to MEMORY\n",
            "2024-04-27 15:27:55,282 INFO reduce.InMemoryMapOutput: Read 98 bytes from map-output for attempt_local474333698_0001_m_000000_0\n",
            "2024-04-27 15:27:55,284 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 98, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->98\n",
            "2024-04-27 15:27:55,288 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2024-04-27 15:27:55,289 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-04-27 15:27:55,289 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2024-04-27 15:27:55,299 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2024-04-27 15:27:55,300 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 90 bytes\n",
            "2024-04-27 15:27:55,301 INFO reduce.MergeManagerImpl: Merged 1 segments, 98 bytes to disk to satisfy reduce memory limit\n",
            "2024-04-27 15:27:55,302 INFO reduce.MergeManagerImpl: Merging 1 files, 102 bytes from disk\n",
            "2024-04-27 15:27:55,302 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2024-04-27 15:27:55,303 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2024-04-27 15:27:55,303 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 90 bytes\n",
            "2024-04-27 15:27:55,304 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-04-27 15:27:55,310 INFO streaming.PipeMapRed: PipeMapRed exec [/content/./reduce.sh]\n",
            "2024-04-27 15:27:55,314 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
            "2024-04-27 15:27:55,316 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
            "2024-04-27 15:27:55,332 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2024-04-27 15:27:55,336 INFO streaming.PipeMapRed: Records R/W=9/1\n",
            "2024-04-27 15:27:55,337 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2024-04-27 15:27:55,338 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2024-04-27 15:27:55,339 INFO mapred.Task: Task:attempt_local474333698_0001_r_000000_0 is done. And is in the process of committing\n",
            "2024-04-27 15:27:55,340 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-04-27 15:27:55,340 INFO mapred.Task: Task attempt_local474333698_0001_r_000000_0 is allowed to commit now\n",
            "2024-04-27 15:27:55,343 INFO output.FileOutputCommitter: Saved output of task 'attempt_local474333698_0001_r_000000_0' to file:/content/wordcount/output\n",
            "2024-04-27 15:27:55,343 INFO mapred.LocalJobRunner: Records R/W=9/1 > reduce\n",
            "2024-04-27 15:27:55,344 INFO mapred.Task: Task 'attempt_local474333698_0001_r_000000_0' done.\n",
            "2024-04-27 15:27:55,344 INFO mapred.Task: Final Counters for attempt_local474333698_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=1429\n",
            "\t\tFILE: Number of bytes written=716724\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=5\n",
            "\t\tReduce shuffle bytes=102\n",
            "\t\tReduce input records=9\n",
            "\t\tReduce output records=5\n",
            "\t\tSpilled Records=9\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=419430400\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=68\n",
            "2024-04-27 15:27:55,345 INFO mapred.LocalJobRunner: Finishing task: attempt_local474333698_0001_r_000000_0\n",
            "2024-04-27 15:27:55,345 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2024-04-27 15:27:55,735 INFO mapreduce.Job: Job job_local474333698_0001 running in uber mode : false\n",
            "2024-04-27 15:27:55,736 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2024-04-27 15:27:55,738 INFO mapreduce.Job: Job job_local474333698_0001 completed successfully\n",
            "2024-04-27 15:27:55,753 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=2622\n",
            "\t\tFILE: Number of bytes written=1433278\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=3\n",
            "\t\tMap output records=9\n",
            "\t\tMap output bytes=78\n",
            "\t\tMap output materialized bytes=102\n",
            "\t\tInput split bytes=92\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=5\n",
            "\t\tReduce shuffle bytes=102\n",
            "\t\tReduce input records=9\n",
            "\t\tReduce output records=5\n",
            "\t\tSpilled Records=18\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=11\n",
            "\t\tTotal committed heap usage (bytes)=838860800\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=76\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=68\n",
            "2024-04-27 15:27:55,753 INFO streaming.StreamJob: Output directory: wordcount/output\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "hdfs dfs -rm -r wordcount/output 2>/dev/null\n",
        "mapred streaming \\\n",
        "  -file map.sh \\\n",
        "  -file reduce.sh \\\n",
        "  -input wordcount/input \\\n",
        "  -output wordcount/output \\\n",
        "  -mapper map.sh \\\n",
        "  -reducer reduce.sh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4rVxTFiD3FV"
      },
      "source": [
        "Let's check the output on the HDFS filesystem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XFa3o02jD3FV",
        "outputId": "bbd07613-5f5e-4032-e017-d5c63cf2acef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "apple \t 2\n",
            "banana \t 1\n",
            "orange \t 1\n",
            "peach \t 4\n",
            "pineapple \t 1\n"
          ]
        }
      ],
      "source": [
        "!hdfs dfs -cat wordcount/output/part*|head"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COUcf-XOD3FW"
      },
      "source": [
        "## Run a mapreduce job with more data <a name=\"moredata\"></a>\n",
        "\n",
        "Let's create a datafile by downloading some real data, for instance from a Web page. This example will be used to introduce some advanced configurations.\n",
        "\n",
        "Next, we download a URL with `wget` and filter out HTML tags with a `sed` regular expression."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "MdvWEhzBD3FW"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "URL=https://www.derstandard.at/story/2000110819049/und-wo-warst-du-beim-fall-der-mauer\n",
        "wget -qO- $URL | sed -e 's/<[^>]*>//g;s/^ //g' >sample_article.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fmW-whJ6D3FW",
        "outputId": "872907c7-2ff5-4db9-c7e9-2000af5dc01a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\t1\n",
            "\r\t1\n",
            "\r\t1\n",
            "\r\t1\n",
            "window.DERSTANDARD.pageConfig.init({\"edition\":\"at\",\"environment\":\"Production\",\"baseUrls\":{\"currentDocument\":\"https://www.derstandard.at\",\"authorization\":\"https://apps.derstandard.at/autorisierung\",\"userprofile\":\"https://apps.derstandard.at/userprofil\",\"staticfiles\":\"https://at.staticfiles.at\"},\"settings\":{\"disableNotifications\":false}})\r\t1\n",
            "\r\t1\n",
            "\r\t1\n",
            "\r\t1\n",
            "Und\t1\n",
            "wo\t1\n"
          ]
        }
      ],
      "source": [
        "!cat sample_article.txt|./map.sh|head"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOaIENfWD3FX"
      },
      "source": [
        "As usual, with real data there's some more work to do. Here we see that the mapper script doesn't skip empty lines. Let's modify it so that empty lines are skipped."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uljg1_teD3FX",
        "outputId": "7a4da1c9-d97a-4580-b1a4-f29119e9b4d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting map.sh\n"
          ]
        }
      ],
      "source": [
        "%%writefile map.sh\n",
        "#!/bin/bash\n",
        "\n",
        "while read line\n",
        "do\n",
        " for word in $line\n",
        " do\n",
        "  if [[ \"$line\" =~ [^[:space:]] ]]\n",
        "  then\n",
        "    if [ -n \"$word\" ]\n",
        "    then\n",
        "    echo -e ${word} \"\\t1\"\n",
        "    fi\n",
        "  fi\n",
        " done\n",
        "done"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bnp1fwzxD3FY",
        "outputId": "1e86c12d-e7e6-43e8-8ba4-310924d7d026"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "window.DERSTANDARD.pageConfig.init({\"edition\":\"at\",\"environment\":\"Production\",\"baseUrls\":{\"currentDocument\":\"https://www.derstandard.at\",\"authorization\":\"https://apps.derstandard.at/autorisierung\",\"userprofile\":\"https://apps.derstandard.at/userprofil\",\"staticfiles\":\"https://at.staticfiles.at\"},\"settings\":{\"disableNotifications\":false}})\r \t1\n",
            "Und \t1\n",
            "wo \t1\n",
            "warst \t1\n",
            "du \t1\n",
            "beim \t1\n",
            "Fall \t1\n",
            "der \t1\n",
            "Mauer? \t1\n",
            "- \t1\n"
          ]
        }
      ],
      "source": [
        "!cat sample_article.txt|./map.sh|head"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvb0h_hDD3FY"
      },
      "source": [
        "Now the output of `map.sh` looks better!\n",
        "\n",
        "<b>Note:</b> when working with real data we need in general some more preprocessing in order to remove control characters or invalid unicode.\n",
        "\n",
        "Time to run MapReduce again with the new data, but first we need to \"put\" the data on HDFS."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sB6QXbf0D3FY",
        "outputId": "181c5325-9238-4f65-c861-e53d36461159"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleted wordcount/input\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "hdfs dfs -rm -r wordcount/input 2>/dev/null\n",
        "hdfs dfs -put sample_article.txt wordcount/input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lk9bvaioD3FZ",
        "outputId": "39e935a6-086c-47bd-9080-5917c3a1f3d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-rw-r--r--   1 root root     29.0 K 2024-04-27 15:28 wordcount/input\n"
          ]
        }
      ],
      "source": [
        "# check that the folder wordcount/input on HDFS only contains sample_article.txt\n",
        "!hdfs dfs -ls -h wordcount/input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XvAhcSuD3Fa"
      },
      "source": [
        "Check the reducer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2oYQDdc7D3Fa",
        "outputId": "32ec2316-a5bc-4c3c-8165-7a3764748180"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "window.DERSTANDARD.pageConfig.init({\"edition\":\"at\",\"environment\":\"Production\",\"baseUrls\":{\"currentDocument\":\"https://www.derstandard.at\",\"authorization\":\"https://apps.derstandard.at/autorisierung\",\"userprofile\":\"https://apps.derstandard.at/userprofil\",\"staticfiles\":\"https://at.staticfiles.at\"},\"settings\":{\"disableNotifications\":false}})\r \t 1\n",
            "Und \t 1\n",
            "wo \t 1\n",
            "warst \t 1\n",
            "du \t 1\n",
            "beim \t 1\n",
            "Fall \t 1\n",
            "der \t 1\n",
            "Mauer? \t 1\n",
            "- \t 1\n"
          ]
        }
      ],
      "source": [
        "!cat sample_article.txt|./map.sh|./reduce.sh|head"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m_xSCtZ0D3Fa",
        "outputId": "b92a0b64-270f-4041-bf8e-995e9073b2cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleted wordcount/output\n",
            "packageJobJar: [map.sh, reduce.sh] [] /tmp/streamjob17247322926136504322.jar tmpDir=null\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-04-27 15:28:08,412 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
            "2024-04-27 15:28:09,275 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2024-04-27 15:28:09,558 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2024-04-27 15:28:09,558 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2024-04-27 15:28:09,599 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2024-04-27 15:28:10,053 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2024-04-27 15:28:10,108 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2024-04-27 15:28:10,659 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1506478115_0001\n",
            "2024-04-27 15:28:10,660 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2024-04-27 15:28:11,400 INFO mapred.LocalDistributedCacheManager: Localized file:/content/map.sh as file:/tmp/hadoop-root/mapred/local/job_local1506478115_0001_798e3961-4028-4ac2-ad7d-a5440edd0d8b/map.sh\n",
            "2024-04-27 15:28:11,464 INFO mapred.LocalDistributedCacheManager: Localized file:/content/reduce.sh as file:/tmp/hadoop-root/mapred/local/job_local1506478115_0001_d3d64e1b-15d7-4d72-b5dd-51ff1c8a8c18/reduce.sh\n",
            "2024-04-27 15:28:11,743 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2024-04-27 15:28:11,748 INFO mapreduce.Job: Running job: job_local1506478115_0001\n",
            "2024-04-27 15:28:11,757 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2024-04-27 15:28:11,765 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2024-04-27 15:28:11,789 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-04-27 15:28:11,789 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-04-27 15:28:11,879 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2024-04-27 15:28:11,899 INFO mapred.LocalJobRunner: Starting task: attempt_local1506478115_0001_m_000000_0\n",
            "2024-04-27 15:28:11,987 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-04-27 15:28:11,987 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-04-27 15:28:12,042 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-04-27 15:28:12,064 INFO mapred.MapTask: Processing split: file:/content/wordcount/input:0+29720\n",
            "2024-04-27 15:28:12,082 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2024-04-27 15:28:12,179 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2024-04-27 15:28:12,179 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2024-04-27 15:28:12,179 INFO mapred.MapTask: soft limit at 83886080\n",
            "2024-04-27 15:28:12,179 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2024-04-27 15:28:12,179 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2024-04-27 15:28:12,182 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2024-04-27 15:28:12,190 INFO streaming.PipeMapRed: PipeMapRed exec [/content/./map.sh]\n",
            "2024-04-27 15:28:12,195 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2024-04-27 15:28:12,197 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2024-04-27 15:28:12,198 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2024-04-27 15:28:12,198 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2024-04-27 15:28:12,199 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2024-04-27 15:28:12,199 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2024-04-27 15:28:12,201 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2024-04-27 15:28:12,202 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2024-04-27 15:28:12,202 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2024-04-27 15:28:12,202 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2024-04-27 15:28:12,203 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2024-04-27 15:28:12,204 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2024-04-27 15:28:12,231 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2024-04-27 15:28:12,232 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2024-04-27 15:28:12,233 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2024-04-27 15:28:12,247 INFO streaming.PipeMapRed: Records R/W=186/1\n",
            "2024-04-27 15:28:12,516 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2024-04-27 15:28:12,517 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2024-04-27 15:28:12,520 INFO mapred.LocalJobRunner: \n",
            "2024-04-27 15:28:12,520 INFO mapred.MapTask: Starting flush of map output\n",
            "2024-04-27 15:28:12,520 INFO mapred.MapTask: Spilling map output\n",
            "2024-04-27 15:28:12,520 INFO mapred.MapTask: bufstart = 0; bufend = 32527; bufvoid = 104857600\n",
            "2024-04-27 15:28:12,520 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26209816(104839264); length = 4581/6553600\n",
            "2024-04-27 15:28:12,554 INFO mapred.MapTask: Finished spill 0\n",
            "2024-04-27 15:28:12,571 INFO mapred.Task: Task:attempt_local1506478115_0001_m_000000_0 is done. And is in the process of committing\n",
            "2024-04-27 15:28:12,575 INFO mapred.LocalJobRunner: Records R/W=186/1\n",
            "2024-04-27 15:28:12,576 INFO mapred.Task: Task 'attempt_local1506478115_0001_m_000000_0' done.\n",
            "2024-04-27 15:28:12,583 INFO mapred.Task: Final Counters for attempt_local1506478115_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=31159\n",
            "\t\tFILE: Number of bytes written=754875\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=186\n",
            "\t\tMap output records=1146\n",
            "\t\tMap output bytes=32527\n",
            "\t\tMap output materialized bytes=34873\n",
            "\t\tInput split bytes=81\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=1146\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=403701760\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=29968\n",
            "2024-04-27 15:28:12,584 INFO mapred.LocalJobRunner: Finishing task: attempt_local1506478115_0001_m_000000_0\n",
            "2024-04-27 15:28:12,584 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2024-04-27 15:28:12,615 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2024-04-27 15:28:12,616 INFO mapred.LocalJobRunner: Starting task: attempt_local1506478115_0001_r_000000_0\n",
            "2024-04-27 15:28:12,628 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-04-27 15:28:12,628 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-04-27 15:28:12,629 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-04-27 15:28:12,645 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@3906d143\n",
            "2024-04-27 15:28:12,653 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2024-04-27 15:28:12,675 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2382574336, maxSingleShuffleLimit=595643584, mergeThreshold=1572499072, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2024-04-27 15:28:12,681 INFO reduce.EventFetcher: attempt_local1506478115_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2024-04-27 15:28:12,720 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1506478115_0001_m_000000_0 decomp: 34869 len: 34873 to MEMORY\n",
            "2024-04-27 15:28:12,724 INFO reduce.InMemoryMapOutput: Read 34869 bytes from map-output for attempt_local1506478115_0001_m_000000_0\n",
            "2024-04-27 15:28:12,726 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 34869, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->34869\n",
            "2024-04-27 15:28:12,728 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2024-04-27 15:28:12,729 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-04-27 15:28:12,729 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2024-04-27 15:28:12,736 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2024-04-27 15:28:12,736 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 34851 bytes\n",
            "2024-04-27 15:28:12,755 INFO reduce.MergeManagerImpl: Merged 1 segments, 34869 bytes to disk to satisfy reduce memory limit\n",
            "2024-04-27 15:28:12,756 INFO reduce.MergeManagerImpl: Merging 1 files, 34873 bytes from disk\n",
            "2024-04-27 15:28:12,756 INFO mapreduce.Job: Job job_local1506478115_0001 running in uber mode : false\n",
            "2024-04-27 15:28:12,757 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2024-04-27 15:28:12,757 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2024-04-27 15:28:12,757 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "2024-04-27 15:28:12,758 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 34851 bytes\n",
            "2024-04-27 15:28:12,759 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-04-27 15:28:12,766 INFO streaming.PipeMapRed: PipeMapRed exec [/content/./reduce.sh]\n",
            "2024-04-27 15:28:12,771 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
            "2024-04-27 15:28:12,774 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
            "2024-04-27 15:28:12,795 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2024-04-27 15:28:12,795 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2024-04-27 15:28:12,797 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2024-04-27 15:28:12,809 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2024-04-27 15:28:12,812 INFO streaming.PipeMapRed: Records R/W=1146/1\n",
            "2024-04-27 15:28:12,886 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2024-04-27 15:28:12,887 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2024-04-27 15:28:12,890 INFO mapred.Task: Task:attempt_local1506478115_0001_r_000000_0 is done. And is in the process of committing\n",
            "2024-04-27 15:28:12,891 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-04-27 15:28:12,891 INFO mapred.Task: Task attempt_local1506478115_0001_r_000000_0 is allowed to commit now\n",
            "2024-04-27 15:28:12,893 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1506478115_0001_r_000000_0' to file:/content/wordcount/output\n",
            "2024-04-27 15:28:12,894 INFO mapred.LocalJobRunner: Records R/W=1146/1 > reduce\n",
            "2024-04-27 15:28:12,894 INFO mapred.Task: Task 'attempt_local1506478115_0001_r_000000_0' done.\n",
            "2024-04-27 15:28:12,895 INFO mapred.Task: Final Counters for attempt_local1506478115_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=100937\n",
            "\t\tFILE: Number of bytes written=819340\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=736\n",
            "\t\tReduce shuffle bytes=34873\n",
            "\t\tReduce input records=1146\n",
            "\t\tReduce output records=746\n",
            "\t\tSpilled Records=1146\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=403701760\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=29592\n",
            "2024-04-27 15:28:12,895 INFO mapred.LocalJobRunner: Finishing task: attempt_local1506478115_0001_r_000000_0\n",
            "2024-04-27 15:28:12,895 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2024-04-27 15:28:13,758 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2024-04-27 15:28:13,759 INFO mapreduce.Job: Job job_local1506478115_0001 completed successfully\n",
            "2024-04-27 15:28:13,775 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=132096\n",
            "\t\tFILE: Number of bytes written=1574215\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=186\n",
            "\t\tMap output records=1146\n",
            "\t\tMap output bytes=32527\n",
            "\t\tMap output materialized bytes=34873\n",
            "\t\tInput split bytes=81\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=736\n",
            "\t\tReduce shuffle bytes=34873\n",
            "\t\tReduce input records=1146\n",
            "\t\tReduce output records=746\n",
            "\t\tSpilled Records=2292\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=807403520\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=29968\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=29592\n",
            "2024-04-27 15:28:13,775 INFO streaming.StreamJob: Output directory: wordcount/output\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "hadoop fs -rmr wordcount/output 2>/dev/null\n",
        "mapred streaming \\\n",
        "  -file map.sh \\\n",
        "  -file reduce.sh \\\n",
        "  -input wordcount/input \\\n",
        "  -output wordcount/output \\\n",
        "  -mapper map.sh \\\n",
        "  -reducer reduce.sh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWl5JZ8wD3Fb"
      },
      "source": [
        "Check the output on HDFS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uh0m4Zp8D3Fb",
        "outputId": "4d4911a9-5cf0-4c24-ea56-28b4d45c5e8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2 items\n",
            "-rw-r--r--   1 root root          0 2024-04-27 15:28 wordcount/output/_SUCCESS\n",
            "-rw-r--r--   1 root root      29352 2024-04-27 15:28 wordcount/output/part-00000\n"
          ]
        }
      ],
      "source": [
        "!hdfs dfs -ls wordcount/output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70a5eU1pD3Fb"
      },
      "source": [
        "This job took a few seconds and this is quite some time for such a small file (4KB). This is due to the overhead of distributing the data and running the Hadoop framework.\n",
        "The advantage of Hadoop can be appreciated only for large datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ls0Ui7hD3Fb",
        "outputId": "1edf5bb7-d2ca-4c07-87dd-3ee828834d64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!!n.frames[t]; \t 1\n",
            "!0)); \t 1\n",
            "!1)) \t 1\n",
            "!1, \t 1\n",
            "!= \t 1\n",
            "!== \t 2\n",
            "!function \t 2\n",
            "!r \t 1\n",
            "\"'+n+'\"',o)}return{key:r,value:e.substr(t+1)}},t._renewCache=function(){t._cache=t._getCacheFromString(t._document.cookie),t._cachedDocumentCookie=t._document.cookie},t._areEnabled=function(){var \t 1\n",
            "\"))}function \t 1\n",
            "cat: Unable to write to output stream.\n"
          ]
        }
      ],
      "source": [
        "!hdfs dfs -cat wordcount/output/part-00000|head"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wvt3S7uWD3Fb"
      },
      "source": [
        "### Sort the output with `sort` <a name=\"sortoutput\"></a>\n",
        "\n",
        "We've obtained a list of tokens that appear in the file followed by their frequencies.\n",
        "\n",
        "The output of the reducer is sorted by key (the word) because that's the ordering that the reducer becomes from the mapper. If we're interested in sorting the data by frequency, we can use the Unix `sort` command (with the options `k2`, `n`, `r` respectively \"by field 2\", \"numeric\", \"reverse\")."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vBq1abi8D3Fc",
        "outputId": "9b150357-8ef4-43d4-f4a2-1eb4446be884"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "= \t 40\n",
            "{ \t 22\n",
            "var \t 22\n",
            "&& \t 19\n",
            "strict\";function \t 13\n",
            "} \t 12\n",
            "in \t 12\n",
            "not \t 12\n",
            "to \t 10\n",
            "e&&e.__esModule?e:{\"default\":e}}function \t 9\n"
          ]
        }
      ],
      "source": [
        "!hdfs dfs -cat wordcount/output/part-00000|sort -k2nr|head"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M89ZZizrD3Fc"
      },
      "source": [
        "The most common word appears to be \"die\" (the German for the definite article \"the\")."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RyAIHjQdD3Fc"
      },
      "source": [
        "### Sort the output with another MapReduce job <a name=\"sortoutputMR\"></a>\n",
        "\n",
        "If we wanted to sort the output of the reducer using the mapreduce framework, we could employ a simple trick: create a mapper that interchanges words with their frequency values. Since by construction mappers sort their output by key, we get the desired sorting as a side-effect.\n",
        "\n",
        "Call the new mapper `swap_keyval.sh`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xVoMDI8MD3Fc",
        "outputId": "589a453f-e901-4f48-d7bf-8d1780f6f34c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing swap_keyval.sh\n"
          ]
        }
      ],
      "source": [
        "%%writefile swap_keyval.sh\n",
        "#!/bin/bash\n",
        "# This script will read one line at a time and swap key/value\n",
        "# For instance, the line \"word 100\" will become \"100 word\"\n",
        "\n",
        "while read key val\n",
        "do\n",
        " printf \"%s\\t%s\\n\" \"$val\" \"$key\"\n",
        "done"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLkYiRvHD3Fd"
      },
      "source": [
        "We are going to run the swap mapper script on the output of the previous mapreduce job. Note that in the below cell we are not deleting the previous output but instead we're saving the output from the current job in a new folder `output_sorted`.\n",
        "\n",
        "Nice thing about running a job on the output of a preceding job is that we do not need to upload files to HDFS because the data is already on HDFS. Not so nice: writing data to disk at each step of a data transformation pipeline takes time and this can be costly for longer data pipelines. This is one of the shortcomings of MapReduce that are addressed by [Apache Spark](https://spark.apache.org/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qhpbRr5HD3Fd",
        "outputId": "b12f01c1-c270-4419-a64b-ff44ed484fa5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "packageJobJar: [swap_keyval.sh] [] /tmp/streamjob9765986180776063495.jar tmpDir=null\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-04-27 15:28:23,779 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
            "2024-04-27 15:28:25,007 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2024-04-27 15:28:25,266 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2024-04-27 15:28:25,267 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2024-04-27 15:28:25,292 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2024-04-27 15:28:25,540 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2024-04-27 15:28:25,588 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2024-04-27 15:28:25,876 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local970214322_0001\n",
            "2024-04-27 15:28:25,877 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2024-04-27 15:28:26,312 INFO mapred.LocalDistributedCacheManager: Localized file:/content/swap_keyval.sh as file:/tmp/hadoop-root/mapred/local/job_local970214322_0001_ee3a93ce-dd31-4b24-aef1-60daff3af3f6/swap_keyval.sh\n",
            "2024-04-27 15:28:26,457 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2024-04-27 15:28:26,459 INFO mapreduce.Job: Running job: job_local970214322_0001\n",
            "2024-04-27 15:28:26,465 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2024-04-27 15:28:26,468 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2024-04-27 15:28:26,473 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-04-27 15:28:26,473 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-04-27 15:28:26,534 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2024-04-27 15:28:26,539 INFO mapred.LocalJobRunner: Starting task: attempt_local970214322_0001_m_000000_0\n",
            "2024-04-27 15:28:26,581 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-04-27 15:28:26,584 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-04-27 15:28:26,624 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-04-27 15:28:26,645 INFO mapred.MapTask: Processing split: file:/content/wordcount/output/part-00000:0+29352\n",
            "2024-04-27 15:28:26,664 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2024-04-27 15:28:26,745 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2024-04-27 15:28:26,746 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2024-04-27 15:28:26,746 INFO mapred.MapTask: soft limit at 83886080\n",
            "2024-04-27 15:28:26,746 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2024-04-27 15:28:26,746 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2024-04-27 15:28:26,750 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2024-04-27 15:28:26,758 INFO streaming.PipeMapRed: PipeMapRed exec [/content/./swap_keyval.sh]\n",
            "2024-04-27 15:28:26,765 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2024-04-27 15:28:26,768 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2024-04-27 15:28:26,769 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2024-04-27 15:28:26,769 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2024-04-27 15:28:26,771 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2024-04-27 15:28:26,772 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2024-04-27 15:28:26,777 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2024-04-27 15:28:26,777 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2024-04-27 15:28:26,778 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2024-04-27 15:28:26,778 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2024-04-27 15:28:26,779 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2024-04-27 15:28:26,780 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2024-04-27 15:28:26,815 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2024-04-27 15:28:26,815 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2024-04-27 15:28:26,818 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2024-04-27 15:28:26,839 INFO streaming.PipeMapRed: Records R/W=746/1\n",
            "2024-04-27 15:28:26,889 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2024-04-27 15:28:26,890 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2024-04-27 15:28:26,893 INFO mapred.LocalJobRunner: \n",
            "2024-04-27 15:28:26,893 INFO mapred.MapTask: Starting flush of map output\n",
            "2024-04-27 15:28:26,893 INFO mapred.MapTask: Spilling map output\n",
            "2024-04-27 15:28:26,893 INFO mapred.MapTask: bufstart = 0; bufend = 27907; bufvoid = 104857600\n",
            "2024-04-27 15:28:26,893 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26211416(104845664); length = 2981/6553600\n",
            "2024-04-27 15:28:26,911 INFO mapred.MapTask: Finished spill 0\n",
            "2024-04-27 15:28:26,946 INFO mapred.Task: Task:attempt_local970214322_0001_m_000000_0 is done. And is in the process of committing\n",
            "2024-04-27 15:28:26,951 INFO mapred.LocalJobRunner: Records R/W=746/1\n",
            "2024-04-27 15:28:26,951 INFO mapred.Task: Task 'attempt_local970214322_0001_m_000000_0' done.\n",
            "2024-04-27 15:28:26,964 INFO mapred.Task: Final Counters for attempt_local970214322_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=30232\n",
            "\t\tFILE: Number of bytes written=743544\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=746\n",
            "\t\tMap output records=746\n",
            "\t\tMap output bytes=27907\n",
            "\t\tMap output materialized bytes=29453\n",
            "\t\tInput split bytes=93\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=746\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=12\n",
            "\t\tTotal committed heap usage (bytes)=364904448\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=29596\n",
            "2024-04-27 15:28:26,964 INFO mapred.LocalJobRunner: Finishing task: attempt_local970214322_0001_m_000000_0\n",
            "2024-04-27 15:28:26,964 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2024-04-27 15:28:26,981 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2024-04-27 15:28:26,982 INFO mapred.LocalJobRunner: Starting task: attempt_local970214322_0001_r_000000_0\n",
            "2024-04-27 15:28:27,013 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-04-27 15:28:27,013 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-04-27 15:28:27,014 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-04-27 15:28:27,017 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@52cfe873\n",
            "2024-04-27 15:28:27,022 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2024-04-27 15:28:27,042 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2382574336, maxSingleShuffleLimit=595643584, mergeThreshold=1572499072, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2024-04-27 15:28:27,048 INFO reduce.EventFetcher: attempt_local970214322_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2024-04-27 15:28:27,096 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local970214322_0001_m_000000_0 decomp: 29449 len: 29453 to MEMORY\n",
            "2024-04-27 15:28:27,101 INFO reduce.InMemoryMapOutput: Read 29449 bytes from map-output for attempt_local970214322_0001_m_000000_0\n",
            "2024-04-27 15:28:27,106 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 29449, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->29449\n",
            "2024-04-27 15:28:27,111 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2024-04-27 15:28:27,112 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-04-27 15:28:27,113 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2024-04-27 15:28:27,120 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2024-04-27 15:28:27,120 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 29445 bytes\n",
            "2024-04-27 15:28:27,134 INFO reduce.MergeManagerImpl: Merged 1 segments, 29449 bytes to disk to satisfy reduce memory limit\n",
            "2024-04-27 15:28:27,134 INFO reduce.MergeManagerImpl: Merging 1 files, 29453 bytes from disk\n",
            "2024-04-27 15:28:27,135 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2024-04-27 15:28:27,135 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2024-04-27 15:28:27,138 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 29445 bytes\n",
            "2024-04-27 15:28:27,139 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-04-27 15:28:27,162 INFO mapred.Task: Task:attempt_local970214322_0001_r_000000_0 is done. And is in the process of committing\n",
            "2024-04-27 15:28:27,164 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-04-27 15:28:27,164 INFO mapred.Task: Task attempt_local970214322_0001_r_000000_0 is allowed to commit now\n",
            "2024-04-27 15:28:27,166 INFO output.FileOutputCommitter: Saved output of task 'attempt_local970214322_0001_r_000000_0' to file:/content/wordcount/output2\n",
            "2024-04-27 15:28:27,167 INFO mapred.LocalJobRunner: reduce > reduce\n",
            "2024-04-27 15:28:27,167 INFO mapred.Task: Task 'attempt_local970214322_0001_r_000000_0' done.\n",
            "2024-04-27 15:28:27,167 INFO mapred.Task: Final Counters for attempt_local970214322_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=89170\n",
            "\t\tFILE: Number of bytes written=801085\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=15\n",
            "\t\tReduce shuffle bytes=29453\n",
            "\t\tReduce input records=746\n",
            "\t\tReduce output records=746\n",
            "\t\tSpilled Records=746\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=364904448\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=28088\n",
            "2024-04-27 15:28:27,168 INFO mapred.LocalJobRunner: Finishing task: attempt_local970214322_0001_r_000000_0\n",
            "2024-04-27 15:28:27,168 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2024-04-27 15:28:27,464 INFO mapreduce.Job: Job job_local970214322_0001 running in uber mode : false\n",
            "2024-04-27 15:28:27,466 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2024-04-27 15:28:27,467 INFO mapreduce.Job: Job job_local970214322_0001 completed successfully\n",
            "2024-04-27 15:28:27,478 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=119402\n",
            "\t\tFILE: Number of bytes written=1544629\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=746\n",
            "\t\tMap output records=746\n",
            "\t\tMap output bytes=27907\n",
            "\t\tMap output materialized bytes=29453\n",
            "\t\tInput split bytes=93\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=15\n",
            "\t\tReduce shuffle bytes=29453\n",
            "\t\tReduce input records=746\n",
            "\t\tReduce output records=746\n",
            "\t\tSpilled Records=1492\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=12\n",
            "\t\tTotal committed heap usage (bytes)=729808896\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=29596\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=28088\n",
            "2024-04-27 15:28:27,478 INFO streaming.StreamJob: Output directory: wordcount/output2\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "hdfs dfs -rm -r wordcount/output2 2>/dev/null\n",
        "mapred streaming \\\n",
        "  -file swap_keyval.sh \\\n",
        "  -input wordcount/output \\\n",
        "  -output wordcount/output2 \\\n",
        "  -mapper swap_keyval.sh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGa-qTjkD3Fd"
      },
      "source": [
        "Check the output on HDFS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IDj-FrWZD3Fd",
        "outputId": "26919db8-25d9-49ef-8379-529937c7d9ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2 items\n",
            "-rw-r--r--   1 root root          0 2024-04-27 15:28 wordcount/output2/_SUCCESS\n",
            "-rw-r--r--   1 root root      27860 2024-04-27 15:28 wordcount/output2/part-00000\n"
          ]
        }
      ],
      "source": [
        "!hdfs dfs -ls wordcount/output2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OZs75WfwD3Fd",
        "outputId": "15c81c9c-b96d-4a05-baca-3c15fce9c55b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\t!!n.frames[t];\n",
            "1\tberraschen.\n",
            "1\tber\n",
            "1\t\n",
            "1\t},\n",
            "1\t}();\n",
            "1\t}(),\n",
            "1\t{};\n",
            "1\ty(){E[\"default\"].debug(\"User\n",
            "1\ty(),j(),void(ne=D());case\n",
            "cat: Unable to write to output stream.\n"
          ]
        }
      ],
      "source": [
        "!hdfs dfs -cat wordcount/output2/part-00000|head"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwU4MGGVD3Fe"
      },
      "source": [
        "Mapper uses by default ascending order to sort by key. We could have changed that with an option but for now let's look at the end of the file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "COXboJaHD3Fe",
        "outputId": "32cb5a58-e3c8-485e-ed02-e7e0c027b142"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7\t==\n",
            "7\t:\n",
            "7\tdie\n",
            "7\t?\n",
            "7\tif\n",
            "7\ttypeof\n",
            "8\t0\n",
            "8\tn(e){return\n",
            "9\te&&e.__esModule?e:{\"default\":e}}function\n",
            "9\tr\n"
          ]
        }
      ],
      "source": [
        "!hdfs dfs -cat wordcount/output2/part-00000|tail"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QRPxpgtD3Fe"
      },
      "source": [
        "### Configure sort with `KeyFieldBasedComparator` <a name=\"KeyFieldBasedComparator\"></a>\n",
        "\n",
        "In general, we can determine how mappers are going to sort their output by configuring the comparator directive to use the special class [`KeyFieldBasedComparator`](https://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapreduce/lib/partition/KeyFieldBasedComparator.html)\n",
        "<html><pre>-D mapreduce.job.output.key.comparator.class=\\\n",
        "    org.apache.hadoop.mapred.lib.KeyFieldBasedComparator</pre></html>\n",
        "    \n",
        "This class has some options similar to the Unix `sort`(`-n` to sort numerically, `-r` for reverse sorting, `-k pos1[,pos2]` for specifying fields to sort by).\n",
        "\n",
        "Let us see the comparator in action on our data to get the desired result. Note that this time we are removing `output2` because we're running the second mapreduce job again."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yckLOf7dD3Fe",
        "outputId": "ca1c6a17-3a4f-4560-85d1-b08b03aa1f8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleted wordcount/output2\n",
            "packageJobJar: [swap_keyval.sh] [] /tmp/streamjob17243880953663811135.jar tmpDir=null\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-04-27 15:28:37,679 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
            "2024-04-27 15:28:38,688 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2024-04-27 15:28:38,888 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2024-04-27 15:28:38,888 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2024-04-27 15:28:38,914 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2024-04-27 15:28:39,166 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2024-04-27 15:28:39,192 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2024-04-27 15:28:39,486 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1581283664_0001\n",
            "2024-04-27 15:28:39,486 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2024-04-27 15:28:39,936 INFO mapred.LocalDistributedCacheManager: Localized file:/content/swap_keyval.sh as file:/tmp/hadoop-root/mapred/local/job_local1581283664_0001_05c6d098-450f-4019-a8bb-8ea38b9df253/swap_keyval.sh\n",
            "2024-04-27 15:28:40,087 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2024-04-27 15:28:40,088 INFO mapreduce.Job: Running job: job_local1581283664_0001\n",
            "2024-04-27 15:28:40,094 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2024-04-27 15:28:40,096 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2024-04-27 15:28:40,102 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-04-27 15:28:40,102 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-04-27 15:28:40,178 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2024-04-27 15:28:40,183 INFO mapred.LocalJobRunner: Starting task: attempt_local1581283664_0001_m_000000_0\n",
            "2024-04-27 15:28:40,229 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-04-27 15:28:40,232 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-04-27 15:28:40,266 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-04-27 15:28:40,282 INFO mapred.MapTask: Processing split: file:/content/wordcount/output/part-00000:0+29352\n",
            "2024-04-27 15:28:40,298 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2024-04-27 15:28:40,376 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2024-04-27 15:28:40,377 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2024-04-27 15:28:40,377 INFO mapred.MapTask: soft limit at 83886080\n",
            "2024-04-27 15:28:40,377 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2024-04-27 15:28:40,377 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2024-04-27 15:28:40,382 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2024-04-27 15:28:40,391 INFO streaming.PipeMapRed: PipeMapRed exec [/content/./swap_keyval.sh]\n",
            "2024-04-27 15:28:40,397 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2024-04-27 15:28:40,402 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2024-04-27 15:28:40,402 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2024-04-27 15:28:40,403 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2024-04-27 15:28:40,403 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2024-04-27 15:28:40,404 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2024-04-27 15:28:40,406 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2024-04-27 15:28:40,406 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2024-04-27 15:28:40,407 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2024-04-27 15:28:40,407 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2024-04-27 15:28:40,408 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2024-04-27 15:28:40,409 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2024-04-27 15:28:40,434 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2024-04-27 15:28:40,434 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2024-04-27 15:28:40,436 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2024-04-27 15:28:40,446 INFO streaming.PipeMapRed: Records R/W=746/1\n",
            "2024-04-27 15:28:40,478 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2024-04-27 15:28:40,479 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2024-04-27 15:28:40,485 INFO mapred.LocalJobRunner: \n",
            "2024-04-27 15:28:40,485 INFO mapred.MapTask: Starting flush of map output\n",
            "2024-04-27 15:28:40,485 INFO mapred.MapTask: Spilling map output\n",
            "2024-04-27 15:28:40,485 INFO mapred.MapTask: bufstart = 0; bufend = 27907; bufvoid = 104857600\n",
            "2024-04-27 15:28:40,485 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26211416(104845664); length = 2981/6553600\n",
            "2024-04-27 15:28:40,520 INFO mapred.MapTask: Finished spill 0\n",
            "2024-04-27 15:28:40,552 INFO mapred.Task: Task:attempt_local1581283664_0001_m_000000_0 is done. And is in the process of committing\n",
            "2024-04-27 15:28:40,556 INFO mapred.LocalJobRunner: Records R/W=746/1\n",
            "2024-04-27 15:28:40,557 INFO mapred.Task: Task 'attempt_local1581283664_0001_m_000000_0' done.\n",
            "2024-04-27 15:28:40,565 INFO mapred.Task: Final Counters for attempt_local1581283664_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=30232\n",
            "\t\tFILE: Number of bytes written=747919\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=746\n",
            "\t\tMap output records=746\n",
            "\t\tMap output bytes=27907\n",
            "\t\tMap output materialized bytes=29453\n",
            "\t\tInput split bytes=93\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=746\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=13\n",
            "\t\tTotal committed heap usage (bytes)=373293056\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=29596\n",
            "2024-04-27 15:28:40,565 INFO mapred.LocalJobRunner: Finishing task: attempt_local1581283664_0001_m_000000_0\n",
            "2024-04-27 15:28:40,565 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2024-04-27 15:28:40,582 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2024-04-27 15:28:40,585 INFO mapred.LocalJobRunner: Starting task: attempt_local1581283664_0001_r_000000_0\n",
            "2024-04-27 15:28:40,618 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-04-27 15:28:40,619 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-04-27 15:28:40,619 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-04-27 15:28:40,625 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@190072db\n",
            "2024-04-27 15:28:40,628 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2024-04-27 15:28:40,649 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2382574336, maxSingleShuffleLimit=595643584, mergeThreshold=1572499072, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2024-04-27 15:28:40,651 INFO reduce.EventFetcher: attempt_local1581283664_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2024-04-27 15:28:40,691 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1581283664_0001_m_000000_0 decomp: 29449 len: 29453 to MEMORY\n",
            "2024-04-27 15:28:40,695 INFO reduce.InMemoryMapOutput: Read 29449 bytes from map-output for attempt_local1581283664_0001_m_000000_0\n",
            "2024-04-27 15:28:40,698 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 29449, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->29449\n",
            "2024-04-27 15:28:40,701 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2024-04-27 15:28:40,702 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-04-27 15:28:40,702 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2024-04-27 15:28:40,709 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2024-04-27 15:28:40,710 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 29444 bytes\n",
            "2024-04-27 15:28:40,720 INFO reduce.MergeManagerImpl: Merged 1 segments, 29449 bytes to disk to satisfy reduce memory limit\n",
            "2024-04-27 15:28:40,720 INFO reduce.MergeManagerImpl: Merging 1 files, 29453 bytes from disk\n",
            "2024-04-27 15:28:40,721 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2024-04-27 15:28:40,721 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2024-04-27 15:28:40,722 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 29444 bytes\n",
            "2024-04-27 15:28:40,723 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-04-27 15:28:40,756 INFO mapred.Task: Task:attempt_local1581283664_0001_r_000000_0 is done. And is in the process of committing\n",
            "2024-04-27 15:28:40,757 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-04-27 15:28:40,757 INFO mapred.Task: Task attempt_local1581283664_0001_r_000000_0 is allowed to commit now\n",
            "2024-04-27 15:28:40,759 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1581283664_0001_r_000000_0' to file:/content/wordcount/output2\n",
            "2024-04-27 15:28:40,760 INFO mapred.LocalJobRunner: reduce > reduce\n",
            "2024-04-27 15:28:40,760 INFO mapred.Task: Task 'attempt_local1581283664_0001_r_000000_0' done.\n",
            "2024-04-27 15:28:40,761 INFO mapred.Task: Final Counters for attempt_local1581283664_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=89170\n",
            "\t\tFILE: Number of bytes written=805460\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=15\n",
            "\t\tReduce shuffle bytes=29453\n",
            "\t\tReduce input records=746\n",
            "\t\tReduce output records=746\n",
            "\t\tSpilled Records=746\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=373293056\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=28088\n",
            "2024-04-27 15:28:40,761 INFO mapred.LocalJobRunner: Finishing task: attempt_local1581283664_0001_r_000000_0\n",
            "2024-04-27 15:28:40,761 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2024-04-27 15:28:41,093 INFO mapreduce.Job: Job job_local1581283664_0001 running in uber mode : false\n",
            "2024-04-27 15:28:41,094 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2024-04-27 15:28:41,096 INFO mapreduce.Job: Job job_local1581283664_0001 completed successfully\n",
            "2024-04-27 15:28:41,109 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=119402\n",
            "\t\tFILE: Number of bytes written=1553379\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=746\n",
            "\t\tMap output records=746\n",
            "\t\tMap output bytes=27907\n",
            "\t\tMap output materialized bytes=29453\n",
            "\t\tInput split bytes=93\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=15\n",
            "\t\tReduce shuffle bytes=29453\n",
            "\t\tReduce input records=746\n",
            "\t\tReduce output records=746\n",
            "\t\tSpilled Records=1492\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=13\n",
            "\t\tTotal committed heap usage (bytes)=746586112\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=29596\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=28088\n",
            "2024-04-27 15:28:41,110 INFO streaming.StreamJob: Output directory: wordcount/output2\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "hdfs dfs -rmr wordcount/output2 2>/dev/null\n",
        "comparator_class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator\n",
        "mapred streaming \\\n",
        "  -D mapreduce.job.output.key.comparator.class=$comparator_class \\\n",
        "  -D mapreduce.partition.keycomparator.options=-nr \\\n",
        "  -file swap_keyval.sh \\\n",
        "  -input wordcount/output \\\n",
        "  -output wordcount/output2 \\\n",
        "  -mapper swap_keyval.sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yh2cIb9mD3Fe",
        "outputId": "100770ad-393c-4553-8240-285e90d3708c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2 items\n",
            "-rw-r--r--   1 root root          0 2024-04-27 15:28 wordcount/output2/_SUCCESS\n",
            "-rw-r--r--   1 root root      27860 2024-04-27 15:28 wordcount/output2/part-00000\n"
          ]
        }
      ],
      "source": [
        "!hdfs dfs -ls wordcount/output2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eWxjdQ7hD3Fe",
        "outputId": "9e8075d9-4f6c-4f21-9b62-30f7e61b6488"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "40\t=\n",
            "22\t{\n",
            "22\tvar\n",
            "19\t&&\n",
            "13\tstrict\";function\n",
            "12\tnot\n",
            "12\t}\n",
            "12\tin\n",
            "10\tto\n",
            "9\te&&e.__esModule?e:{\"default\":e}}function\n",
            "cat: Unable to write to output stream.\n"
          ]
        }
      ],
      "source": [
        "!hdfs dfs -cat wordcount/output2/part-00000|head"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APPySwzzD3Ff"
      },
      "source": [
        "Now we get the output in the desired order."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alzQgJAnD3Fi"
      },
      "source": [
        "### Specifying Configuration Variables with the -D Option <a name=\"configuration_variables\"></a>\n",
        "\n",
        "With the `-D` option it is possible to override options set in the default configuration file [`mapred_default.xml`](https://hadoop.apache.org/docs/current/hadoop-mapreduce-client/hadoop-mapreduce-client-core/mapred-default.xml)\n",
        "(see the [Apache Hadoop documentation](https://hadoop.apache.org/docs/current/hadoop-streaming/HadoopStreaming.html#Specifying_Configuration_Variables_with_the_-D_Option)).\n",
        "\n",
        "One option that might come handy when dealing with out-of-memory issues in the sorting phase is the size in MB of the memory reserved for sorting `mapreduce.task.io.sort.mb`:\n",
        " <html>\n",
        "    <pre>-D mapreduce.task.io.sort.mb=512\n",
        "    </pre>\n",
        " </html>\n",
        "\n",
        " **Note:** the maximum value for `mapreduce.task.io.sort.mb` is 2047.   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wYKD9aID3Fi"
      },
      "source": [
        "## What is word count useful for? <a name=\"wordcount\"></a>\n",
        "Counting the frequencies of words is at the basis of _indexing_ and it facilitates the retrieval of relevant documents in search engines."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}