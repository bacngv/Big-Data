{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bacngv/Big-Data/blob/main/MapReduce%20Tutorials/simplest_mapreduce_bash_wordcount.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4iIw-8oyIy-f"
      },
      "source": [
        "# Simple distributed wordcount with MapReduce"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wT6lnV46Iy-g"
      },
      "source": [
        "Check that file `file.txt` exists, view size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6vyHKMVBIy-g",
        "outputId": "9a7e3c26-5a56-465c-aafd-6f7b4da5caa8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-rw-r--r-- 1 root root 4.9K Apr 27 15:41 file.txt\n"
          ]
        }
      ],
      "source": [
        "!ls -hal file.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "HADOOP_URL = \"https://dlcdn.apache.org/hadoop/common/stable/hadoop-3.4.0.tar.gz\"\n",
        "\n",
        "import requests\n",
        "import os\n",
        "import tarfile\n",
        "\n",
        "def download_and_extract_targz(url):\n",
        "    response = requests.get(url)\n",
        "    filename = url.rsplit('/', 1)[-1]\n",
        "    HADOOP_HOME = filename[:-7]\n",
        "    # set HADOOP_HOME environment variable\n",
        "    os.environ['HADOOP_HOME'] = HADOOP_HOME\n",
        "    if os.path.isdir(HADOOP_HOME):\n",
        "      print(\"Not downloading, Hadoop folder {} already exists\".format(HADOOP_HOME))\n",
        "      return\n",
        "    if response.status_code == 200:\n",
        "        with open(filename, 'wb') as file:\n",
        "            file.write(response.content)\n",
        "        with tarfile.open(filename, 'r:gz') as tar_ref:\n",
        "            extract_path = tar_ref.extractall(path='.')\n",
        "            # Get the names of all members (files and directories) in the archive\n",
        "            all_members = tar_ref.getnames()\n",
        "            # If there is a top-level directory, get its name\n",
        "            if all_members:\n",
        "              top_level_directory = all_members[0]\n",
        "              print(f\"ZIP file downloaded and extracted successfully. Contents saved at: {top_level_directory}\")\n",
        "    else:\n",
        "        print(f\"Failed to download ZIP file. Status code: {response.status_code}\")\n",
        "\n",
        "\n",
        "download_and_extract_targz(HADOOP_URL)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iVxW55wJI-Js",
        "outputId": "d0e587b4-88af-410d-9b59-4fb16035a2b3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ZIP file downloaded and extracted successfully. Contents saved at: hadoop-3.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# HADOOP_HOME was set earlier when downloading Hadoop distribution\n",
        "print(\"HADOOP_HOME is {}\".format(os.environ['HADOOP_HOME']))\n",
        "\n",
        "os.environ['PATH'] = ':'.join([os.path.join(os.environ['HADOOP_HOME'], 'bin'), os.environ['PATH']])\n",
        "print(\"PATH is {}\".format(os.environ['PATH']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_LrmNl2JBbr",
        "outputId": "4751044e-dfa4-40aa-ea6e-4feb7112b5a9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HADOOP_HOME is hadoop-3.4.0\n",
            "PATH is hadoop-3.4.0/bin:/opt/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "# set variable JAVA_HOME (install Java if necessary)\n",
        "def is_java_installed():\n",
        "    os.environ['JAVA_HOME'] = os.path.realpath(shutil.which(\"java\")).split('/bin')[0]\n",
        "    return os.environ['JAVA_HOME']\n",
        "\n",
        "def install_java():\n",
        "    # Uncomment and modify the desired version\n",
        "    # java_version= 'openjdk-11-jre-headless'\n",
        "    # java_version= 'default-jre'\n",
        "    # java_version= 'openjdk-17-jre-headless'\n",
        "    # java_version= 'openjdk-18-jre-headless'\n",
        "    java_version= 'openjdk-19-jre-headless'\n",
        "\n",
        "    print(f\"Java not found. Installing {java_version} ... (this might take a while)\")\n",
        "    try:\n",
        "        cmd = f\"apt install -y {java_version}\"\n",
        "        subprocess_output = subprocess.run(cmd, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
        "        stdout_result = subprocess_output.stdout\n",
        "        # Process the results as needed\n",
        "        print(\"Done installing Java {}\".format(java_version))\n",
        "        os.environ['JAVA_HOME'] = os.path.realpath(shutil.which(\"java\")).split('/bin')[0]\n",
        "        print(\"JAVA_HOME is {}\".format(os.environ['JAVA_HOME']))\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        # Handle the error if the command returns a non-zero exit code\n",
        "        print(\"Command failed with return code {}\".format(e.returncode))\n",
        "        print(\"stdout: {}\".format(e.stdout))\n",
        "\n",
        "# Install Java if not available\n",
        "if is_java_installed():\n",
        "    print(\"Java is already installed: {}\".format(os.environ['JAVA_HOME']))\n",
        "else:\n",
        "    print(\"Installing Java\")\n",
        "    install_java()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HyDtjvG-JDTZ",
        "outputId": "aab5e799-f688-4caa-c424-a25c3f03feec"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Java is already installed: /usr/lib/jvm/java-11-openjdk-amd64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a73JxYdnIy-g"
      },
      "source": [
        "Copy file to HDFS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Nt9PehYYIy-h"
      },
      "outputs": [],
      "source": [
        "!hdfs dfs -put -f file.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtztjvZrIy-h"
      },
      "source": [
        "Erase `result` folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "tpdbbm1MIy-h"
      },
      "outputs": [],
      "source": [
        "!hdfs dfs -rm -R result 2>/dev/null"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9rqcZQrIy-h"
      },
      "source": [
        "Run the bash wordcount command `wc` in parallel on the distributed file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99_t2pSKIy-h",
        "outputId": "3e9a8d86-6445-40a8-f3a1-ae095d9b708d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-04-27 15:43:15,843 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2024-04-27 15:43:16,231 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2024-04-27 15:43:16,231 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2024-04-27 15:43:16,278 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2024-04-27 15:43:16,868 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2024-04-27 15:43:16,926 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2024-04-27 15:43:17,563 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1245727715_0001\n",
            "2024-04-27 15:43:17,563 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2024-04-27 15:43:18,084 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2024-04-27 15:43:18,086 INFO mapreduce.Job: Running job: job_local1245727715_0001\n",
            "2024-04-27 15:43:18,121 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2024-04-27 15:43:18,130 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2024-04-27 15:43:18,140 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-04-27 15:43:18,140 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-04-27 15:43:18,199 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2024-04-27 15:43:18,212 INFO mapred.LocalJobRunner: Starting task: attempt_local1245727715_0001_m_000000_0\n",
            "2024-04-27 15:43:18,251 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-04-27 15:43:18,254 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-04-27 15:43:18,293 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-04-27 15:43:18,326 INFO mapred.MapTask: Processing split: file:/content/file.txt:0+4921\n",
            "2024-04-27 15:43:18,351 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2024-04-27 15:43:18,443 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2024-04-27 15:43:18,443 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2024-04-27 15:43:18,443 INFO mapred.MapTask: soft limit at 83886080\n",
            "2024-04-27 15:43:18,443 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2024-04-27 15:43:18,443 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2024-04-27 15:43:18,446 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2024-04-27 15:43:18,450 INFO streaming.PipeMapRed: PipeMapRed exec [/bin/cat]\n",
            "2024-04-27 15:43:18,457 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2024-04-27 15:43:18,461 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2024-04-27 15:43:18,461 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2024-04-27 15:43:18,462 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2024-04-27 15:43:18,463 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2024-04-27 15:43:18,463 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2024-04-27 15:43:18,466 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2024-04-27 15:43:18,466 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2024-04-27 15:43:18,466 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2024-04-27 15:43:18,467 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2024-04-27 15:43:18,468 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2024-04-27 15:43:18,469 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2024-04-27 15:43:18,518 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2024-04-27 15:43:18,518 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2024-04-27 15:43:18,522 INFO streaming.PipeMapRed: Records R/W=44/1\n",
            "2024-04-27 15:43:18,525 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2024-04-27 15:43:18,526 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2024-04-27 15:43:18,532 INFO mapred.LocalJobRunner: \n",
            "2024-04-27 15:43:18,532 INFO mapred.MapTask: Starting flush of map output\n",
            "2024-04-27 15:43:18,532 INFO mapred.MapTask: Spilling map output\n",
            "2024-04-27 15:43:18,532 INFO mapred.MapTask: bufstart = 0; bufend = 4966; bufvoid = 104857600\n",
            "2024-04-27 15:43:18,532 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214224(104856896); length = 173/6553600\n",
            "2024-04-27 15:43:18,548 INFO mapred.MapTask: Finished spill 0\n",
            "2024-04-27 15:43:18,564 INFO mapred.Task: Task:attempt_local1245727715_0001_m_000000_0 is done. And is in the process of committing\n",
            "2024-04-27 15:43:18,569 INFO mapred.LocalJobRunner: Records R/W=44/1\n",
            "2024-04-27 15:43:18,569 INFO mapred.Task: Task 'attempt_local1245727715_0001_m_000000_0' done.\n",
            "2024-04-27 15:43:18,578 INFO mapred.Task: Final Counters for attempt_local1245727715_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=146878\n",
            "\t\tFILE: Number of bytes written=864356\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=44\n",
            "\t\tMap output records=44\n",
            "\t\tMap output bytes=4966\n",
            "\t\tMap output materialized bytes=5061\n",
            "\t\tInput split bytes=74\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=44\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=372244480\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=4977\n",
            "2024-04-27 15:43:18,579 INFO mapred.LocalJobRunner: Finishing task: attempt_local1245727715_0001_m_000000_0\n",
            "2024-04-27 15:43:18,580 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2024-04-27 15:43:18,584 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2024-04-27 15:43:18,585 INFO mapred.LocalJobRunner: Starting task: attempt_local1245727715_0001_r_000000_0\n",
            "2024-04-27 15:43:18,607 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-04-27 15:43:18,607 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-04-27 15:43:18,608 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-04-27 15:43:18,624 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@50b73b4c\n",
            "2024-04-27 15:43:18,635 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2024-04-27 15:43:18,659 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2382574336, maxSingleShuffleLimit=595643584, mergeThreshold=1572499072, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2024-04-27 15:43:18,661 INFO reduce.EventFetcher: attempt_local1245727715_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2024-04-27 15:43:18,708 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1245727715_0001_m_000000_0 decomp: 5057 len: 5061 to MEMORY\n",
            "2024-04-27 15:43:18,716 INFO reduce.InMemoryMapOutput: Read 5057 bytes from map-output for attempt_local1245727715_0001_m_000000_0\n",
            "2024-04-27 15:43:18,718 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 5057, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->5057\n",
            "2024-04-27 15:43:18,722 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2024-04-27 15:43:18,724 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-04-27 15:43:18,724 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2024-04-27 15:43:18,736 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2024-04-27 15:43:18,736 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 4936 bytes\n",
            "2024-04-27 15:43:18,739 INFO reduce.MergeManagerImpl: Merged 1 segments, 5057 bytes to disk to satisfy reduce memory limit\n",
            "2024-04-27 15:43:18,739 INFO reduce.MergeManagerImpl: Merging 1 files, 5061 bytes from disk\n",
            "2024-04-27 15:43:18,740 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2024-04-27 15:43:18,740 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2024-04-27 15:43:18,741 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 4936 bytes\n",
            "2024-04-27 15:43:18,742 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-04-27 15:43:18,743 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/bin/wc]\n",
            "2024-04-27 15:43:18,748 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
            "2024-04-27 15:43:18,751 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
            "2024-04-27 15:43:18,772 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2024-04-27 15:43:18,772 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2024-04-27 15:43:18,777 INFO streaming.PipeMapRed: Records R/W=44/1\n",
            "2024-04-27 15:43:18,779 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2024-04-27 15:43:18,780 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2024-04-27 15:43:18,781 INFO mapred.Task: Task:attempt_local1245727715_0001_r_000000_0 is done. And is in the process of committing\n",
            "2024-04-27 15:43:18,785 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-04-27 15:43:18,789 INFO mapred.Task: Task attempt_local1245727715_0001_r_000000_0 is allowed to commit now\n",
            "2024-04-27 15:43:18,798 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1245727715_0001_r_000000_0' to file:/content/result\n",
            "2024-04-27 15:43:18,799 INFO mapred.LocalJobRunner: Records R/W=44/1 > reduce\n",
            "2024-04-27 15:43:18,799 INFO mapred.Task: Task 'attempt_local1245727715_0001_r_000000_0' done.\n",
            "2024-04-27 15:43:18,800 INFO mapred.Task: Final Counters for attempt_local1245727715_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=157032\n",
            "\t\tFILE: Number of bytes written=869454\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=44\n",
            "\t\tReduce shuffle bytes=5061\n",
            "\t\tReduce input records=44\n",
            "\t\tReduce output records=1\n",
            "\t\tSpilled Records=44\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=372244480\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=37\n",
            "2024-04-27 15:43:18,800 INFO mapred.LocalJobRunner: Finishing task: attempt_local1245727715_0001_r_000000_0\n",
            "2024-04-27 15:43:18,800 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2024-04-27 15:43:19,093 INFO mapreduce.Job: Job job_local1245727715_0001 running in uber mode : false\n",
            "2024-04-27 15:43:19,094 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2024-04-27 15:43:19,096 INFO mapreduce.Job: Job job_local1245727715_0001 completed successfully\n",
            "2024-04-27 15:43:19,105 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=303910\n",
            "\t\tFILE: Number of bytes written=1733810\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=44\n",
            "\t\tMap output records=44\n",
            "\t\tMap output bytes=4966\n",
            "\t\tMap output materialized bytes=5061\n",
            "\t\tInput split bytes=74\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=44\n",
            "\t\tReduce shuffle bytes=5061\n",
            "\t\tReduce input records=44\n",
            "\t\tReduce output records=1\n",
            "\t\tSpilled Records=88\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=744488960\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=4977\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=37\n",
            "2024-04-27 15:43:19,106 INFO streaming.StreamJob: Output directory: result\n"
          ]
        }
      ],
      "source": [
        "!mapred streaming \\\n",
        "  -input file.txt \\\n",
        "  -output result \\\n",
        "  -mapper /bin/cat \\\n",
        "  -reducer /usr/bin/wc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTPqmBGeIy-i"
      },
      "source": [
        "Check result of MapReduce job"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WmquUffXIy-i",
        "outputId": "3613dfac-cba5-41f1-f7b8-fa6cbf6bd454"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     44     810    4965\t\n"
          ]
        }
      ],
      "source": [
        "!hdfs dfs -cat result/part*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5kCAxd7Iy-i"
      },
      "source": [
        "Check that the word count is correct by comparing with `wc` on local host (warning: do not try with too large files)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lF-pVICoIy-i",
        "outputId": "282ba96c-87bc-4106-b144-185fc4c49584"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  43  810 4921 file.txt\n"
          ]
        }
      ],
      "source": [
        "!wc file.txt"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}